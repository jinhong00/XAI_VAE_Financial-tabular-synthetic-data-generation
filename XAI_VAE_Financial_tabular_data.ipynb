{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare\n",
        "\n",
        "Before you start this notebook. Complete the following steps:\n",
        "\n",
        "1. create a dir in the /content, named by \"data\"\n",
        "2. upload the csv file to \"/content/data\""
      ],
      "metadata": {
        "id": "mR_i1-IRZRVq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwb5xyeZeCu6"
      },
      "outputs": [],
      "source": [
        "!pip install category-encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpYEYN5rp0Rk"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import tensorflow.compat.v1 as tf # to make it work with tf2.0\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import sklearn\n",
        "from numpy.random import seed # to randomize sensitive client information\n",
        "from numpy.random import randint # to randomize sensitive client information\n",
        "from category_encoders import TargetEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "tf.enable_eager_execution() \n",
        "train_loss = tf.keras.metrics.Mean()\n",
        "val_loss = tf.keras.metrics.Mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmKsrkjNp57W"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Yer638Qp7ZS"
      },
      "outputs": [],
      "source": [
        "TRAIN_BUF = 100000 \n",
        "BATCH_SIZE = 512\n",
        "INPUT_DIM=19\n",
        "COMPRESSION_FACTOR=(3/4)\n",
        "LATENT_DIM= int(COMPRESSION_FACTOR*INPUT_DIM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMNVgKz4bRCA"
      },
      "source": [
        "# Data process\n",
        "\n",
        "This part is used to pre-process the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7ecJ7Zfbeko"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_df = train_data.groupby(train_data.columns[-1])\n",
        "grouped_df.size().plot(kind='bar',color=[(0.2,0.4,0.6),(0.8,0.5,0)])\n",
        "\n",
        "plt.xlabel('Subscribe to a term deposit')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.xticks(range(len(grouped_df.groups)), ['no', 'yes'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "25tqpoLdM5ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Plou-j0ebes4"
      },
      "outputs": [],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9G0xQdK2bev0"
      },
      "outputs": [],
      "source": [
        "train_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL-TW6qvcKHE"
      },
      "outputs": [],
      "source": [
        "category_col = [col for col in train_data.columns.tolist() if train_data[col].dtype == object]\n",
        "print(category_col)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "JgnojjjvSGEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in category_col:\n",
        "    lc = LabelEncoder()\n",
        "    train_data[col] = lc.fit_transform(train_data[col])"
      ],
      "metadata": {
        "id": "5uuFGCbuSJyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.info()"
      ],
      "metadata": {
        "id": "SY2L3MWoVSCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_data.drop(\"y\", axis=1)\n",
        "y = train_data['y']"
      ],
      "metadata": {
        "id": "MaJ63aBFSbTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caMTPPY3guTE"
      },
      "source": [
        "## Scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uA3m4JI0d-jY"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTqhp8n_d-m4"
      },
      "outputs": [],
      "source": [
        "x = MinMaxScaler().fit_transform(X)\n",
        "x = pd.DataFrame(data=x, columns=X.columns.tolist())\n",
        "x.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ny23Ola6hdzU"
      },
      "outputs": [],
      "source": [
        "# Save the processed data to csv file\n",
        "x.to_csv(\"/content/data/processed_data.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8HLp_WTSznS"
      },
      "outputs": [],
      "source": [
        "y.to_csv(\"/content/data/y.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJTMIbxjqo3x"
      },
      "source": [
        "# load processed data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    training_set = None\n",
        "    with open(\"/content/data/processed_data.csv\", mode=\"r\") as f:\n",
        "        training_set = pd.read_csv(f)\n",
        "    return training_set\n",
        "\n",
        "\n",
        "def create_complete_data():\n",
        "    training_set = load_data()\n",
        "    size = training_set.shape\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices(tf.constant(training_set.values, dtype = tf.float32, shape=size))\n",
        "    input = train_dataset\n",
        "    train_dataset = train_dataset.shuffle(TRAIN_BUF)\n",
        "    train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "    return train_dataset"
      ],
      "metadata": {
        "id": "q7pU1a8oE8zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHbI43ZbqWZu"
      },
      "source": [
        "# Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HERhoWplqQH-"
      },
      "outputs": [],
      "source": [
        "class TVAE(tf.keras.Model):\n",
        "    def __init__(self, latent_dim, input_dim):\n",
        "        super(TVAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.inference_net = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
        "            tf.keras.layers.Dense(units=input_dim, activation=tf.nn.tanh),\n",
        "            tf.keras.layers.Dense(units=latent_dim, activation=tf.nn.relu),\n",
        "            tf.keras.layers.Dense(units=latent_dim + latent_dim),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "        self.generative_net = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
        "            tf.keras.layers.Dense(units=latent_dim, activation=tf.nn.relu),\n",
        "            tf.keras.layers.Dense(units=input_dim, activation=tf.nn.tanh),\n",
        "            tf.keras.layers.Dense(units=input_dim),\n",
        "            \n",
        "        ]\n",
        "    )\n",
        "\n",
        "    def sample(self, eps=None):\n",
        "        if eps is None:\n",
        "            eps = tf.random_normal(shape=(100, self.latent_dim))\n",
        "        return self.decode(eps, apply_sigmoid=True)\n",
        "\n",
        "    def encode(self, x):\n",
        "        model_output = self.inference_net(x)\n",
        "        mean, logvar = tf.split(model_output, num_or_size_splits=2, axis=1)\n",
        "        return mean, logvar\n",
        "\n",
        "    def reparameterize(self, mean, logvar):\n",
        "        eps = tf.random_normal(shape=mean.shape)\n",
        "        return eps * tf.exp(logvar * .5) + mean\n",
        "\n",
        "    def decode(self, z, apply_sigmoid=False):\n",
        "        logits = self.generative_net(z)\n",
        "        if apply_sigmoid:\n",
        "            probs = tf.sigmoid(logits)\n",
        "            return probs\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or2HRwkhqZNf"
      },
      "source": [
        "# Computing Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUTgpfwMqb8w"
      },
      "outputs": [],
      "source": [
        "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
        "    log2pi = tf.log(2. * np.pi)\n",
        "    return tf.reduce_sum(\n",
        "        -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
        "        axis=raxis)\n",
        "\n",
        "def mean_squared_error(logit, ground_truth):\n",
        "    error = (ground_truth - logit)\n",
        "    MSE = tf.reduce_mean(tf.square(error), axis=1)\n",
        "    return MSE\n",
        "\n",
        "def compute_loss(model, x):\n",
        "    mean, logvar = model.encode(x)\n",
        "   \n",
        "    z = model.reparameterize(mean, logvar)\n",
        "    x_logit = model.decode(z)\n",
        "    x = tf.reshape(x, x_logit.shape)\n",
        "\n",
        "    MSE = mean_squared_error(logit=x_logit, ground_truth=x)\n",
        "    logpx_z = -tf.reduce_sum(MSE, axis=0)\n",
        "\n",
        "    logpz = log_normal_pdf(z, 0., 0.)\n",
        "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
        "    \n",
        "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
        "\n",
        "def compute_gradients(model, x):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = compute_loss(model, x)\n",
        "    return tape.gradient(loss, model.trainable_variables), loss\n",
        "\n",
        "def apply_gradients(optimizer, gradients, variables, global_step=None):\n",
        "    optimizer.apply_gradients(zip(gradients, variables))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QO7aRwHqeEm"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data, feature_num, epochs):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        data: (DataSet) train data set\n",
        "        feature_num: (int) number of feature\n",
        "        removed_feature: (str) feature not used. \n",
        "        epochs: (int)\n",
        "    \"\"\"\n",
        "    epochs = epochs   \n",
        "    latent_dim = LATENT_DIM\n",
        "    num_examples_to_generate = 16\n",
        "    random_vector_for_generation = tf.random_normal(\n",
        "        shape=[num_examples_to_generate, latent_dim]\n",
        "    )\n",
        "    model = TVAE(latent_dim, feature_num)\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(0.01)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        start_time = time.time()\n",
        "        for train_x in data:\n",
        "            gradients, loss = compute_gradients(model, train_x)\n",
        "            apply_gradients(optimizer, gradients, model.trainable_variables)\n",
        "        end_time = time.time()\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            for train_sample_x in data.make_one_shot_iterator():\n",
        "                train_loss(compute_loss(model, train_sample_x))\n",
        "            train_elbo = train_loss.result()\n",
        "            print(f'Epoch: {epoch}, duration: {end_time - start_time:.4f}s, train loss: {train_elbo}')\n",
        "    \n",
        "    x = pd.read_csv(\"/content/data/processed_data.csv\")\n",
        "    x = tf.convert_to_tensor(x.values)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "fi9RmQFC70PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = create_complete_data()\n",
        "model = train(data, feature_num=19, epochs=300)"
      ],
      "metadata": {
        "id": "69eFlcP76UBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretations"
      ],
      "metadata": {
        "id": "fLrVagAHBUJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Steps 1-6 are for feature importance\n",
        "#### Step 1"
      ],
      "metadata": {
        "id": "u4uXfCgYxLnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mus = []\n",
        "logvars = []\n",
        "mu_grads = []\n",
        "logvars_grads = []\n",
        "\n",
        "for train_x in data:\n",
        "    with tf.GradientTape(persistent=True) as g:\n",
        "        g.watch(train_x)\n",
        "        mean, logvar = model.encode(train_x)\n",
        "        mus.append(mean)\n",
        "        logvars.append(logvar)\n",
        "\n",
        "        mu_i_grads = []\n",
        "        logvars_i_grads = []\n",
        "        for i in range(mean.shape[1]):\n",
        "            mu_i_grads.append(g.gradient(mean[:, i], train_x))\n",
        "            logvars_i_grads.append(g.gradient(logvar[:, i], train_x))\n",
        "        \n",
        "        mu_grads.append(mu_i_grads)\n",
        "        logvars_grads.append(logvars_i_grads)"
      ],
      "metadata": {
        "id": "NCl8Kmiq7WvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2 "
      ],
      "metadata": {
        "id": "Jg5slWMSxOH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sj_mu_xn_list = []\n",
        "sj_sigma_xn_list = []\n",
        "\n",
        "for i in range(len(mu_grads)):\n",
        "   \n",
        "    abs_sij_mu_xn = tf.abs(mu_grads[i][0])\n",
        "    abs_sij_sigma_xn = tf.abs(logvars_grads[i][0])\n",
        "    \n",
        "    for j in range(1, len(mu_grads[i])):\n",
        "        abs_sij_mu_xn += tf.abs(mu_grads[i][j])\n",
        "        abs_sij_sigma_xn += tf.abs(logvars_grads[i][j])\n",
        "    \n",
        "    sj_mu_xn_list.append(abs_sij_mu_xn)\n",
        "    sj_sigma_xn_list.append(abs_sij_sigma_xn)\n",
        "\n",
        "sj_mu_xn = tf.concat(sj_mu_xn_list, axis=0)\n",
        "sj_sigma_xn = tf.concat(sj_sigma_xn_list, axis=0)\n",
        "\n",
        "print(f\"shape of mu_grad_x: {sj_mu_xn.shape}\")\n",
        "print(f\"shape of logvar_grad_x: {sj_sigma_xn.shape}\")"
      ],
      "metadata": {
        "id": "F6iCUNYrt2Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sj_xn = sj_mu_xn + sj_sigma_xn\n",
        "print(sj_xn.shape)"
      ],
      "metadata": {
        "id": "iGcW47mOwg4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_grad_normalize_in(idx):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        idx (int): index of data point\n",
        "    \"\"\"\n",
        "    # normalization\n",
        "    sj_xn_normalize = sj_xn / tf.reduce_sum(sj_xn, axis=1, keepdims=True)\n",
        "    grad_in_idx = sj_xn_normalize[idx, :]\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    plt.bar(list(range(1, len(grad_in_idx)+1)), grad_in_idx)\n",
        "    plt.xticks(list(range(1, len(grad_in_idx)+1)))\n",
        "    plt.show()\n",
        "\n",
        "def plot_grad_in(idx):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        idx (int): index of data point\n",
        "    \"\"\"\n",
        "    grad_in_idx = sj_xn[idx, :]\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    plt.bar(list(range(1, len(grad_in_idx)+1)), grad_in_idx)\n",
        "    plt.xticks(list(range(1, len(grad_in_idx)+1)))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "fnLj1prSnXG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#relative local feature importance\n",
        "plot_grad_normalize_in(1)"
      ],
      "metadata": {
        "id": "ja_TMncxqgpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#local feature importance\n",
        "plot_grad_in(1)"
      ],
      "metadata": {
        "id": "wuf4OYggnLlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3"
      ],
      "metadata": {
        "id": "b57eJXnQxVE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sij_mu_xn_square = []\n",
        "sij_sigma_xn_square = []\n",
        "\n",
        "for j in range(len(mu_grads[0])):\n",
        "    \n",
        "    # the element in the following list is grad of a batch data\n",
        "    sij_mu_j_xn_square = []\n",
        "    sij_sigma_j_xn_square = []\n",
        "\n",
        "    for i in range(0, len(mu_grads)):\n",
        "        sij_mu_j_xn_square.append(mu_grads[i][j] ** 2)\n",
        "        sij_sigma_j_xn_square.append(logvars_grads[i][j] ** 2)\n",
        "    \n",
        "    sij_mu_xn_square.append(sij_mu_j_xn_square)\n",
        "    sij_sigma_xn_square.append(sij_sigma_j_xn_square)"
      ],
      "metadata": {
        "id": "ArEGSMbNrPzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sij_mu_sq = []\n",
        "sij_sigma_sq = []\n",
        "for j in range(len(sij_mu_xn_square)):\n",
        "    tmp_mu = tf.concat(sij_mu_xn_square[j], axis=0)\n",
        "    tmp_logvar = tf.concat(sij_sigma_xn_square[j], axis=0)\n",
        "\n",
        "    sij_mu_sq.append(tf.math.sqrt(tf.reduce_mean(tmp_mu, axis=0, keep_dims=True)))\n",
        "    sij_sigma_sq.append(tf.math.sqrt(tf.reduce_mean(tmp_logvar, axis=0, keep_dims=True)))"
      ],
      "metadata": {
        "id": "3XPrqeSAzjAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4"
      ],
      "metadata": {
        "id": "bzrTo-0B1M-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sj_mu_sq = tf.reduce_sum(tf.concat(sij_mu_sq, axis=0), axis=0, keepdims=True)\n",
        "sj_sigma_sq = tf.reduce_sum(tf.concat(sij_sigma_sq, axis=0), axis=0, keepdims=True)"
      ],
      "metadata": {
        "id": "jHsHEgl51Ei7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 5"
      ],
      "metadata": {
        "id": "Pe-_EIjs1vjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sj_sq = sj_mu_sq + sj_sigma_sq"
      ],
      "metadata": {
        "id": "DCn7-wZg1xCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# global feature importance\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "plt.bar(list(range(19)), sj_sq.numpy().reshape(-1))\n",
        "plt.xticks(list(range(19)), [str(i) for i in range(1, 20)])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mxd59lCr15Pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 6"
      ],
      "metadata": {
        "id": "u4Wp9QD_2WW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# relative global feature importance\n",
        "a = sj_sq.numpy().reshape(-1)\n",
        "f = (a / a.sum()) * 100\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "plt.bar(list(range(19)), f)\n",
        "plt.xticks(list(range(19)), [str(i) for i in range(1, 20)])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qWFnN7xJ2YiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 7 Feature interaction"
      ],
      "metadata": {
        "id": "w5XHn2E_7U_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = []\n",
        "for it in data:\n",
        "    train_x.append(it)\n",
        "train_x = tf.concat(train_x, axis=0)\n",
        "print(f\"shape of train_x: {train_x.shape}\")"
      ],
      "metadata": {
        "id": "nyuqg6HWD7zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 7.1"
      ],
      "metadata": {
        "id": "ZT2ztpy39OeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.GradientTape(persistent=True) as g:\n",
        "    with tf.GradientTape(persistent=True) as g2:\n",
        "        g.watch(train_x)\n",
        "        g2.watch(train_x)\n",
        "        mean, logvar = model.encode(train_x)\n",
        "\n",
        "        mjo_xn = []\n",
        "        it = 0    # just a temp variable\n",
        "        for i in range(mean.shape[1]):\n",
        "        \n",
        "            # first order grad\n",
        "            dmu_i_divide_dxon = g.gradient(mean[:, i], train_x)\n",
        "            dsigma_i_divide_dxon = g.gradient(logvar[:, i], train_x)\n",
        "        \n",
        "            # second order grad\n",
        "            it += tf.abs(g2.batch_jacobian(dmu_i_divide_dxon, train_x))\n",
        "            it += tf.abs(g2.batch_jacobian(dsigma_i_divide_dxon, train_x))\n",
        "            mjo_xn.append(it)"
      ],
      "metadata": {
        "id": "kofswlAd7WqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_jj_xn = 0\n",
        "\n",
        "for m in mjo_xn:\n",
        "    m_jj_xn += m\n",
        "\n",
        "print(f\"shape of m_jj_xn {m_jj_xn.shape}\")"
      ],
      "metadata": {
        "id": "tEWpTyPIYcOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_jj_reduce_mean = tf.reduce_mean(m_jj_xn, axis=0)\n",
        "\n",
        "mask = 1 - tf.eye(m_jj_reduce_mean.shape[0].value)\n",
        "\n",
        "m_jo = tf.reshape(m_jj_reduce_mean[tf.cast(mask, bool)], (1, -1))\n",
        "print(f\"shape of m_jo is: {m_jo.shape}\")"
      ],
      "metadata": {
        "id": "w5xmE8mNbDg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 7.2"
      ],
      "metadata": {
        "id": "v8rDkTz-fJhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m_j_xn = tf.reduce_sum(m_jj_xn * tf.expand_dims((1 - tf.eye(m_jj_xn.shape[1].value)), axis=0), axis=2)\n",
        "print(f\"shape of m_j_xn: {m_j_xn.shape}\")"
      ],
      "metadata": {
        "id": "7mIkPbpifMFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_mjxn_in(idx):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        idx (int): index of data point\n",
        "    \"\"\"\n",
        "    grad_in_idx = m_j_xn.numpy()[idx, :]\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    plt.bar(list(range(1, len(grad_in_idx)+1)), grad_in_idx)\n",
        "    plt.xticks(list(range(1, len(grad_in_idx)+1)))\n",
        "    plt.show()\n",
        "\n",
        "plot_mjxn_in(1)"
      ],
      "metadata": {
        "id": "eIhANXZG5CNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_j = tf.reduce_mean(m_j_xn, axis=0)\n",
        "print(f\"shape of m_j: {m_j.shape}\")"
      ],
      "metadata": {
        "id": "KvcN7S04fi2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10, 8))\n",
        "plt.bar(list(range(len(m_j))), m_j)\n",
        "plt.xticks(list(range(19)), [str(i+1) for i in range(19)])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RzMkJudYgPn7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}